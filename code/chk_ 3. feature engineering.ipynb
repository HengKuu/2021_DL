{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0992a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/chku/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, roc_auc_score\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3981bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6978bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>media count</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>num_words</th>\n",
       "      <th>Page Content</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>ymd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>world</td>\n",
       "      <td>Wed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-19 15:04:30</td>\n",
       "      <td>536</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt; &lt;span c...</td>\n",
       "      <td>-1</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>2013-06-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>2</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-03-28 17:40:55</td>\n",
       "      <td>305</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>2013-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>2</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>25</td>\n",
       "      <td>2014-05-07 19:15:20</td>\n",
       "      <td>1011</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>2014-05-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                              topic        channel  \\\n",
       "0   0  Asteroid Asteroids challenge Earth Space U.S. ...          world   \n",
       "1   1  Apps and Software Google open source opn pledg...           tech   \n",
       "2   2      Entertainment NFL NFL Draft Sports Television  entertainment   \n",
       "\n",
       "  weekday            author  img count  \\\n",
       "0     Wed               NaN          1   \n",
       "1     Thu  Christina Warren          2   \n",
       "2     Wed         Sam Laird          2   \n",
       "\n",
       "                                               title  media count  \\\n",
       "0  NASA's Grand Challenge: Stop Asteroids From De...            0   \n",
       "1  Google's New Open Source Patent Pledge: We Won...            0   \n",
       "2  Ballin': 2014 NFL Draft Picks Get to Choose Th...           25   \n",
       "\n",
       "              pub_date  num_words  \\\n",
       "0  2013-06-19 15:04:30        536   \n",
       "1  2013-03-28 17:40:55        305   \n",
       "2  2014-05-07 19:15:20       1011   \n",
       "\n",
       "                                        Page Content  Popularity  \\\n",
       "0  <html><head><div class=\"article-info\"> <span c...          -1   \n",
       "1  <html><head><div class=\"article-info\"><span cl...           1   \n",
       "2  <html><head><div class=\"article-info\"><span cl...           1   \n",
       "\n",
       "   day_of_month  month  day_of_week  hour         ymd  \n",
       "0            19      6            2    15  2013-06-19  \n",
       "1            28      3            3    17  2013-03-28  \n",
       "2             7      5            2    19  2014-05-07  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>media count</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>num_words</th>\n",
       "      <th>Page Content</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>ymd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27643</td>\n",
       "      <td>Entertainment Music One Direction soccer Sports</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Mon</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>1</td>\n",
       "      <td>Soccer Star Gets Twitter Death Threats After T...</td>\n",
       "      <td>7</td>\n",
       "      <td>2013-09-09 19:47:02</td>\n",
       "      <td>475</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2013-09-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27644</td>\n",
       "      <td>Gadgets glass Google Google Glass Google Glass...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Stan Schroeder</td>\n",
       "      <td>3</td>\n",
       "      <td>Google Glass Gets an Accessory Store</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-10-31 09:25:02</td>\n",
       "      <td>142</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2013-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27645</td>\n",
       "      <td>amazon amazon kindle Business Gaming</td>\n",
       "      <td>business</td>\n",
       "      <td>Tue</td>\n",
       "      <td>Todd Wasserman</td>\n",
       "      <td>2</td>\n",
       "      <td>OUYA Gaming Console Already Sold Out on Amazon</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-25 12:54:54</td>\n",
       "      <td>164</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2013-06-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id                                              topic        channel  \\\n",
       "0  27643    Entertainment Music One Direction soccer Sports  entertainment   \n",
       "1  27644  Gadgets glass Google Google Glass Google Glass...           tech   \n",
       "2  27645               amazon amazon kindle Business Gaming       business   \n",
       "\n",
       "  weekday          author  img count  \\\n",
       "0     Mon       Sam Laird          1   \n",
       "1     Thu  Stan Schroeder          3   \n",
       "2     Tue  Todd Wasserman          2   \n",
       "\n",
       "                                               title  media count  \\\n",
       "0  Soccer Star Gets Twitter Death Threats After T...            7   \n",
       "1               Google Glass Gets an Accessory Store            0   \n",
       "2     OUYA Gaming Console Already Sold Out on Amazon            0   \n",
       "\n",
       "              pub_date  num_words  \\\n",
       "0  2013-09-09 19:47:02        475   \n",
       "1  2013-10-31 09:25:02        142   \n",
       "2  2013-06-25 12:54:54        164   \n",
       "\n",
       "                                        Page Content  day_of_month  month  \\\n",
       "0  <html><head><div class=\"article-info\"><span cl...             9      9   \n",
       "1  <html><head><div class=\"article-info\"><span cl...            31     10   \n",
       "2  <html><head><div class=\"article-info\"><span cl...            25      6   \n",
       "\n",
       "   day_of_week  hour         ymd  \n",
       "0            0    19  2013-09-09  \n",
       "1            3     9  2013-10-31  \n",
       "2            1    12  2013-06-25  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load data\n",
    "df_train = pd.read_csv('../chk_output/train_feature.csv')\n",
    "df_test = pd.read_csv('../chk_output/test_feature.csv')\n",
    "\n",
    "display(df_train.head(3))\n",
    "display(df_test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "254bd183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new features by observing EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d999a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a144aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## articles published at weekends seems more popular\n",
    "df_train['is_weekend'] = df_train['day_of_week'].apply(lambda x: 1 if x==5 or x==6 else 0)\n",
    "df_test['is_weekend'] = df_test['day_of_week'].apply(lambda x: 1 if x==5 or x==6 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f460d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09756133419798459"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.corr()['is_weekend']['Popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe2972d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# month\n",
    "def pop_month(x):\n",
    "    if x == 3:\n",
    "        return 1\n",
    "    elif x == 10:\n",
    "        return -1 # -1 means not popular\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_train['popular_month'] = df_train['month'].apply(pop_month)\n",
    "df_test['popular_month'] = df_test['month'].apply(pop_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "decf7eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06495173808218499"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.corr()['popular_month']['Popularity']\n",
    "# I should normalize 'Popularity' to have correct corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7760c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hours\n",
    "def pop_hour(x):\n",
    "    if x == 13 or x == 21:\n",
    "        return -1\n",
    "    elif x == 5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_train['popular_hour'] = df_train['hour'].apply(pop_hour)\n",
    "df_test['popular_hour'] = df_test['hour'].apply(pop_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ade629f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014911374378094084"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.corr()['popular_hour']['Popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76e36858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel \n",
    "# just watch the EDA and assign weights\n",
    "def pop_channel(x):\n",
    "    if x == 'social-media' or x == 'tech':\n",
    "        return 2\n",
    "    elif x == 'marketing' or x == 'lifestyle':\n",
    "        return 1\n",
    "    elif x == 'world' or x == 'entertainment':\n",
    "        return -2\n",
    "    elif x == 'business':\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_train['popular_channel'] = df_train['channel'].apply(pop_channel)\n",
    "df_test['popular_channel'] = df_test['hour'].apply(pop_channel)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e85e6522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>media count</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>num_words</th>\n",
       "      <th>...</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>ymd</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>popular_month</th>\n",
       "      <th>popular_hour</th>\n",
       "      <th>popular_channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>world</td>\n",
       "      <td>Wed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-19 15:04:30</td>\n",
       "      <td>536</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>2013-06-19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>2</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-03-28 17:40:55</td>\n",
       "      <td>305</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>2013-03-28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>2</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>25</td>\n",
       "      <td>2014-05-07 19:15:20</td>\n",
       "      <td>1011</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sports Video Videos Watercooler</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>1</td>\n",
       "      <td>Cameraperson Fails Deliver Slapstick Laughs</td>\n",
       "      <td>21</td>\n",
       "      <td>2013-10-11 02:26:50</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-10-11</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Entertainment instagram instagram video NFL Sp...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Connor Finnegan</td>\n",
       "      <td>52</td>\n",
       "      <td>NFL Star Helps Young Fan Prove Friendship With...</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-17 03:31:43</td>\n",
       "      <td>182</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-04-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                              topic        channel  \\\n",
       "0   0  Asteroid Asteroids challenge Earth Space U.S. ...          world   \n",
       "1   1  Apps and Software Google open source opn pledg...           tech   \n",
       "2   2      Entertainment NFL NFL Draft Sports Television  entertainment   \n",
       "3   3                    Sports Video Videos Watercooler    watercooler   \n",
       "4   4  Entertainment instagram instagram video NFL Sp...  entertainment   \n",
       "\n",
       "  weekday            author  img count  \\\n",
       "0     Wed               NaN          1   \n",
       "1     Thu  Christina Warren          2   \n",
       "2     Wed         Sam Laird          2   \n",
       "3     Fri         Sam Laird          1   \n",
       "4     Thu   Connor Finnegan         52   \n",
       "\n",
       "                                               title  media count  \\\n",
       "0  NASA's Grand Challenge: Stop Asteroids From De...            0   \n",
       "1  Google's New Open Source Patent Pledge: We Won...            0   \n",
       "2  Ballin': 2014 NFL Draft Picks Get to Choose Th...           25   \n",
       "3        Cameraperson Fails Deliver Slapstick Laughs           21   \n",
       "4  NFL Star Helps Young Fan Prove Friendship With...            1   \n",
       "\n",
       "              pub_date  num_words  ... Popularity  day_of_month  month  \\\n",
       "0  2013-06-19 15:04:30        536  ...         -1            19      6   \n",
       "1  2013-03-28 17:40:55        305  ...          1            28      3   \n",
       "2  2014-05-07 19:15:20       1011  ...          1             7      5   \n",
       "3  2013-10-11 02:26:50        187  ...         -1            11     10   \n",
       "4  2014-04-17 03:31:43        182  ...         -1            17      4   \n",
       "\n",
       "   day_of_week  hour         ymd is_weekend  popular_month  popular_hour  \\\n",
       "0            2    15  2013-06-19          0              0             0   \n",
       "1            3    17  2013-03-28          0              1             0   \n",
       "2            2    19  2014-05-07          0              0             0   \n",
       "3            4     2  2013-10-11          0             -1             0   \n",
       "4            3     3  2014-04-17          0              0             0   \n",
       "\n",
       "   popular_channel  \n",
       "0               -2  \n",
       "1                2  \n",
       "2               -2  \n",
       "3                0  \n",
       "4               -2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check\n",
    "print(df_train['popular_channel'].nunique())\n",
    "display(df_train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "120e28d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2668\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# create ordinal features for author\n",
    "print(df_train['author'].isna().sum())\n",
    "print(df_train['channel'].isna().sum())\n",
    "df_train['author'].fillna('N/A', inplace=True) # fill nan author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24a6f2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_popularity_author(col):\n",
    "    df = df_train.groupby(f'{col}').mean().reset_index().sort_values(by='Popularity', ascending=False) \\\n",
    "              [[f'{col}', 'Popularity']]\n",
    "    df.columns=[f'{col}', 'avg_popularity']\n",
    "    \n",
    "    '''\n",
    "    pop_5 = df[df['avg_popularity'] >= 0.7][f'{col}'].values\n",
    "    pop_4 = df[(df['avg_popularity'] < 0.7) & (df['avg_popularity'] >= 0.6)][f'{col}'].values\n",
    "    pop_3 = df[(df['avg_popularity'] < 0.6) & (df['avg_popularity'] >= 0.5)][f'{col}'].values\n",
    "    pop_2 = df[(df['avg_popularity'] < 0.5) & (df['avg_popularity'] >= 0.4)][f'{col}'].values\n",
    "    pop_1 = df[(df['avg_popularity'] < 0.4) & (df['avg_popularity'] >= 0.3)][f'{col}'].values\n",
    "    pop_0 = df[df['avg_popularity'] < 0.3][f'{col}'].values\n",
    "    '''\n",
    "    pop_5 = df[df['avg_popularity'] >= 0.5][f'{col}'].values\n",
    "    pop_2 = df[(df['avg_popularity'] >= 0.2) & (df['avg_popularity'] < 0.5)][f'{col}'].values\n",
    "    unpop_2 = df[(df['avg_popularity'] <= -0.2) & (df['avg_popularity'] >= -0.5)][f'{col}'].values\n",
    "    unpop_5 = df[df['avg_popularity'] < -0.5][f'{col}'].values\n",
    "    \n",
    "    def lambda_fxn(x):\n",
    "        '''\n",
    "        if x in pop_5:\n",
    "            return 5\n",
    "        elif x in pop_4:\n",
    "            return 4\n",
    "        elif x in pop_3:\n",
    "            return 3\n",
    "        elif x in pop_2:\n",
    "            return 2\n",
    "        elif x in pop_1:\n",
    "            return 1\n",
    "        elif x in pop_0:\n",
    "            return -1\n",
    "            \n",
    "        # To catch news desks/sections/subsections/material in test but not in train\n",
    "        else:\n",
    "            return 0\n",
    "        '''\n",
    "        \n",
    "        if x in pop_5:\n",
    "            return 5\n",
    "        elif x in pop_2:\n",
    "            return 2\n",
    "        elif x in unpop_5:\n",
    "            return -5\n",
    "        elif x in unpop_2:\n",
    "            return -2\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    \n",
    "    df_train[f'popular_{col}'] = df_train[f'{col}'].apply(lambda_fxn)\n",
    "    df_test[f'popular_{col}'] = df_test[f'{col}'].apply(lambda_fxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8ccbc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>media count</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>num_words</th>\n",
       "      <th>...</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>ymd</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>popular_month</th>\n",
       "      <th>popular_hour</th>\n",
       "      <th>popular_channel</th>\n",
       "      <th>popular_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>world</td>\n",
       "      <td>Wed</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-19 15:04:30</td>\n",
       "      <td>536</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>2013-06-19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>2</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-03-28 17:40:55</td>\n",
       "      <td>305</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>2013-03-28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>2</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>25</td>\n",
       "      <td>2014-05-07 19:15:20</td>\n",
       "      <td>1011</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sports Video Videos Watercooler</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>1</td>\n",
       "      <td>Cameraperson Fails Deliver Slapstick Laughs</td>\n",
       "      <td>21</td>\n",
       "      <td>2013-10-11 02:26:50</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-10-11</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Entertainment instagram instagram video NFL Sp...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Connor Finnegan</td>\n",
       "      <td>52</td>\n",
       "      <td>NFL Star Helps Young Fan Prove Friendship With...</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-04-17 03:31:43</td>\n",
       "      <td>182</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-04-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                              topic        channel  \\\n",
       "0   0  Asteroid Asteroids challenge Earth Space U.S. ...          world   \n",
       "1   1  Apps and Software Google open source opn pledg...           tech   \n",
       "2   2      Entertainment NFL NFL Draft Sports Television  entertainment   \n",
       "3   3                    Sports Video Videos Watercooler    watercooler   \n",
       "4   4  Entertainment instagram instagram video NFL Sp...  entertainment   \n",
       "\n",
       "  weekday            author  img count  \\\n",
       "0     Wed               N/A          1   \n",
       "1     Thu  Christina Warren          2   \n",
       "2     Wed         Sam Laird          2   \n",
       "3     Fri         Sam Laird          1   \n",
       "4     Thu   Connor Finnegan         52   \n",
       "\n",
       "                                               title  media count  \\\n",
       "0  NASA's Grand Challenge: Stop Asteroids From De...            0   \n",
       "1  Google's New Open Source Patent Pledge: We Won...            0   \n",
       "2  Ballin': 2014 NFL Draft Picks Get to Choose Th...           25   \n",
       "3        Cameraperson Fails Deliver Slapstick Laughs           21   \n",
       "4  NFL Star Helps Young Fan Prove Friendship With...            1   \n",
       "\n",
       "              pub_date  num_words  ... day_of_month  month  day_of_week  hour  \\\n",
       "0  2013-06-19 15:04:30        536  ...           19      6            2    15   \n",
       "1  2013-03-28 17:40:55        305  ...           28      3            3    17   \n",
       "2  2014-05-07 19:15:20       1011  ...            7      5            2    19   \n",
       "3  2013-10-11 02:26:50        187  ...           11     10            4     2   \n",
       "4  2014-04-17 03:31:43        182  ...           17      4            3     3   \n",
       "\n",
       "          ymd  is_weekend popular_month  popular_hour  popular_channel  \\\n",
       "0  2013-06-19           0             0             0               -2   \n",
       "1  2013-03-28           0             1             0                2   \n",
       "2  2014-05-07           0             0             0               -2   \n",
       "3  2013-10-11           0            -1             0                0   \n",
       "4  2014-04-17           0             0             0               -2   \n",
       "\n",
       "   popular_author  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "map_popularity_author('author')\n",
    "#print(df_train['popular_author'].nunique())\n",
    "display(df_train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15303465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setiment on headline\n",
    "#(ignore now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03e56820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine topic and title\n",
    "df_train['combi_text'] = df_train['topic'] + '. ' + df_train['title']\n",
    "df_train['combi_text2'] = df_train['combi_text'].str.replace(r'[\\!?.]+[\\.]+','.', regex=True) # remove extra punctuation in headline\n",
    "\n",
    "df_test['combi_text'] = df_test['topic'] + '. ' + df_test['title']\n",
    "df_test['combi_text2'] = df_test['combi_text'].str.replace(r'[\\!?.]+[\\.]+','.', regex=True)\n",
    "\n",
    "#display(df_train.head(5)['combi_text'])\n",
    "#display(df_train.head(5)['combi_text2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e7ca092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.271, 'neu': 0.467, 'pos': 0.262, 'compound': -0.296}\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# sentiment on combi_text\n",
    "sia = SIA()\n",
    "print(sia.polarity_scores(df_train['combi_text'][0]))\n",
    "print(df_train['Popularity'][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef1a076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def get_sentiment(row):\n",
    "    sentiment_dict = sia.polarity_scores(row['combi_text'])\n",
    "    row['sentiment_pos'] = sentiment_dict['pos'] # positive\n",
    "    row['sentiment_neu'] = sentiment_dict['neu']\n",
    "    row['sentiment_neg'] = sentiment_dict['neg']\n",
    "    row['sentiment_compound'] = sentiment_dict['compound']\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90104e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "## check if there's NaN in ['combi_text'], or SIA() will report error\n",
    "print(df_train['combi_text'].isna().sum())\n",
    "print(df_train['combi_text2'].isna().sum())\n",
    "df_train['combi_text'].fillna('N/A', inplace=True) # fill nan author\n",
    "df_train['combi_text2'].fillna('N/A', inplace=True) # fill nan author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5705136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## double check\n",
    "print(df_train['combi_text'].isna().sum())\n",
    "print(df_train['combi_text2'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7888865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e523cd2095f843ec8889f957875b4ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27643 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = df_train.progress_apply(get_sentiment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "589460f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "## check if there's NaN in ['combi_text'], or SIA() will report error\n",
    "print(df_test['combi_text'].isna().sum())\n",
    "print(df_test['combi_text2'].isna().sum())\n",
    "df_test['combi_text'].fillna('N/A', inplace=True) # fill nan author\n",
    "df_test['combi_text2'].fillna('N/A', inplace=True) # fill nan author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aabaa511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## double check\n",
    "print(df_train['combi_text'].isna().sum())\n",
    "print(df_train['combi_text2'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b64cc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d46a51fc32148bf92a1b21aba672047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11847 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test data\n",
    "df_test = df_test.progress_apply(get_sentiment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45d77b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>media count</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>num_words</th>\n",
       "      <th>...</th>\n",
       "      <th>popular_month</th>\n",
       "      <th>popular_hour</th>\n",
       "      <th>popular_channel</th>\n",
       "      <th>popular_author</th>\n",
       "      <th>combi_text</th>\n",
       "      <th>combi_text2</th>\n",
       "      <th>sentiment_pos</th>\n",
       "      <th>sentiment_neu</th>\n",
       "      <th>sentiment_neg</th>\n",
       "      <th>sentiment_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15271</th>\n",
       "      <td>15271</td>\n",
       "      <td>happy holidays Social Media Vine Challenge</td>\n",
       "      <td>social-media</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Megan Ranney</td>\n",
       "      <td>2</td>\n",
       "      <td>Your jolliest rule-bending holiday greeting cards</td>\n",
       "      <td>6</td>\n",
       "      <td>2014-12-19 21:09:01</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>happy holidays Social Media Vine Challenge. Yo...</td>\n",
       "      <td>happy holidays Social Media Vine Challenge. Yo...</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.9413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26164</th>\n",
       "      <td>26164</td>\n",
       "      <td>Gift guides holiday party Lifestyle Work &amp; Play</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>Mon</td>\n",
       "      <td>James O'Brien</td>\n",
       "      <td>11</td>\n",
       "      <td>9 gifts for your favorite holiday hosts</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-12-15 19:03:17</td>\n",
       "      <td>552</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Gift guides holiday party Lifestyle Work &amp; Pla...</td>\n",
       "      <td>Gift guides holiday party Lifestyle Work &amp; Pla...</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.9371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26118</th>\n",
       "      <td>26118</td>\n",
       "      <td>Cute funny Sex &amp; Love Photography Watercooler</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Laura Vitto</td>\n",
       "      <td>23</td>\n",
       "      <td>20 Perfect GIFs To Express Your Love</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-07-28 18:30:58</td>\n",
       "      <td>119</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cute funny Sex &amp; Love Photography Watercooler....</td>\n",
       "      <td>Cute funny Sex &amp; Love Photography Watercooler....</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.9584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13838</th>\n",
       "      <td>13838</td>\n",
       "      <td>nobel peace prize Twitter U.S. World</td>\n",
       "      <td>world</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Samantha Murphy</td>\n",
       "      <td>2</td>\n",
       "      <td>Nobel Peace Prize Winner Notified Via Twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-10-11 14:31:19</td>\n",
       "      <td>375</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>nobel peace prize Twitter U.S. World. Nobel Pe...</td>\n",
       "      <td>nobel peace prize Twitter U.S. World. Nobel Pe...</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.9545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13708</th>\n",
       "      <td>13708</td>\n",
       "      <td>adventure Video Videos Watercooler YouTube</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Charlie White</td>\n",
       "      <td>1</td>\n",
       "      <td>'Humans Are Awesome' Video Celebrates Daring A...</td>\n",
       "      <td>12</td>\n",
       "      <td>2013-06-27 20:58:42</td>\n",
       "      <td>288</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>adventure Video Videos Watercooler YouTube. 'H...</td>\n",
       "      <td>adventure Video Videos Watercooler YouTube. 'H...</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.9313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17678</th>\n",
       "      <td>17678</td>\n",
       "      <td>Mobile Tech tweetdeck Twitter World</td>\n",
       "      <td>tech</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Samantha Murphy</td>\n",
       "      <td>2</td>\n",
       "      <td>Teen Says He Exposed TweetDeck Vulnerability b...</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-06-11 20:40:07</td>\n",
       "      <td>309</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Mobile Tech tweetdeck Twitter World. Teen Says...</td>\n",
       "      <td>Mobile Tech tweetdeck Twitter World. Teen Says...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.225</td>\n",
       "      <td>-0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>1847</td>\n",
       "      <td>accessories Android The Future of Travel iPhon...</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Dani Fankhauser</td>\n",
       "      <td>7</td>\n",
       "      <td>6 Bags That Charge Your Devices While You Travel</td>\n",
       "      <td>3</td>\n",
       "      <td>2013-09-19 14:50:10</td>\n",
       "      <td>829</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>accessories Android The Future of Travel iPhon...</td>\n",
       "      <td>accessories Android The Future of Travel iPhon...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>1846</td>\n",
       "      <td>Apps and Software art Dev &amp; Design Fashion Gad...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Adario Strange</td>\n",
       "      <td>4</td>\n",
       "      <td>Camera-Covered Jacket Is the Ultimate in Sarto...</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-10-23 03:30:05</td>\n",
       "      <td>220</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Apps and Software art Dev &amp; Design Fashion Gad...</td>\n",
       "      <td>Apps and Software art Dev &amp; Design Fashion Gad...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17681</th>\n",
       "      <td>17681</td>\n",
       "      <td>Advertising Business Evian Evian Babies Market...</td>\n",
       "      <td>advertising</td>\n",
       "      <td>Mon</td>\n",
       "      <td>Todd Wasserman</td>\n",
       "      <td>1</td>\n",
       "      <td>The Evian Babies Are Back, Sans Rollerskates</td>\n",
       "      <td>2</td>\n",
       "      <td>2013-04-22 13:56:46</td>\n",
       "      <td>236</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Advertising Business Evian Evian Babies Market...</td>\n",
       "      <td>Advertising Business Evian Evian Babies Market...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4358</th>\n",
       "      <td>4358</td>\n",
       "      <td>bluefin tuna Climate deepwater drilling deepwa...</td>\n",
       "      <td>world</td>\n",
       "      <td>Mon</td>\n",
       "      <td>Andrew Freedman</td>\n",
       "      <td>3</td>\n",
       "      <td>Deepwater Horizon Study Finds Crude Oil Harmfu...</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-03-24 21:28:56</td>\n",
       "      <td>1092</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>bluefin tuna Climate deepwater drilling deepwa...</td>\n",
       "      <td>bluefin tuna Climate deepwater drilling deepwa...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.893</td>\n",
       "      <td>0.107</td>\n",
       "      <td>-0.5719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27643 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id                                              topic       channel  \\\n",
       "15271  15271         happy holidays Social Media Vine Challenge  social-media   \n",
       "26164  26164    Gift guides holiday party Lifestyle Work & Play     lifestyle   \n",
       "26118  26118      Cute funny Sex & Love Photography Watercooler   watercooler   \n",
       "13838  13838               nobel peace prize Twitter U.S. World         world   \n",
       "13708  13708         adventure Video Videos Watercooler YouTube   watercooler   \n",
       "...      ...                                                ...           ...   \n",
       "17678  17678                Mobile Tech tweetdeck Twitter World          tech   \n",
       "1847    1847  accessories Android The Future of Travel iPhon...     lifestyle   \n",
       "1846    1846  Apps and Software art Dev & Design Fashion Gad...          tech   \n",
       "17681  17681  Advertising Business Evian Evian Babies Market...   advertising   \n",
       "4358    4358  bluefin tuna Climate deepwater drilling deepwa...         world   \n",
       "\n",
       "      weekday           author  img count  \\\n",
       "15271     Fri     Megan Ranney          2   \n",
       "26164     Mon    James O'Brien         11   \n",
       "26118     Sun      Laura Vitto         23   \n",
       "13838     Fri  Samantha Murphy          2   \n",
       "13708     Thu    Charlie White          1   \n",
       "...       ...              ...        ...   \n",
       "17678     Wed  Samantha Murphy          2   \n",
       "1847      Thu  Dani Fankhauser          7   \n",
       "1846      Thu   Adario Strange          4   \n",
       "17681     Mon   Todd Wasserman          1   \n",
       "4358      Mon  Andrew Freedman          3   \n",
       "\n",
       "                                                   title  media count  \\\n",
       "15271  Your jolliest rule-bending holiday greeting cards            6   \n",
       "26164            9 gifts for your favorite holiday hosts            0   \n",
       "26118               20 Perfect GIFs To Express Your Love            0   \n",
       "13838      Nobel Peace Prize Winner Notified Via Twitter            0   \n",
       "13708  'Humans Are Awesome' Video Celebrates Daring A...           12   \n",
       "...                                                  ...          ...   \n",
       "17678  Teen Says He Exposed TweetDeck Vulnerability b...            0   \n",
       "1847    6 Bags That Charge Your Devices While You Travel            3   \n",
       "1846   Camera-Covered Jacket Is the Ultimate in Sarto...            1   \n",
       "17681       The Evian Babies Are Back, Sans Rollerskates            2   \n",
       "4358   Deepwater Horizon Study Finds Crude Oil Harmfu...            0   \n",
       "\n",
       "                  pub_date  num_words  ... popular_month  popular_hour  \\\n",
       "15271  2014-12-19 21:09:01        120  ...             0            -1   \n",
       "26164  2014-12-15 19:03:17        552  ...             0             0   \n",
       "26118  2013-07-28 18:30:58        119  ...             0             0   \n",
       "13838  2013-10-11 14:31:19        375  ...            -1             0   \n",
       "13708  2013-06-27 20:58:42        288  ...             0             0   \n",
       "...                    ...        ...  ...           ...           ...   \n",
       "17678  2014-06-11 20:40:07        309  ...             0             0   \n",
       "1847   2013-09-19 14:50:10        829  ...             0             0   \n",
       "1846   2014-10-23 03:30:05        220  ...            -1             0   \n",
       "17681  2013-04-22 13:56:46        236  ...             0            -1   \n",
       "4358   2014-03-24 21:28:56       1092  ...             1            -1   \n",
       "\n",
       "       popular_channel  popular_author  \\\n",
       "15271                2               0   \n",
       "26164                1               5   \n",
       "26118                0               0   \n",
       "13838               -2               0   \n",
       "13708                0               2   \n",
       "...                ...             ...   \n",
       "17678                2               0   \n",
       "1847                 1               0   \n",
       "1846                 2               0   \n",
       "17681                0               0   \n",
       "4358                -2               0   \n",
       "\n",
       "                                              combi_text  \\\n",
       "15271  happy holidays Social Media Vine Challenge. Yo...   \n",
       "26164  Gift guides holiday party Lifestyle Work & Pla...   \n",
       "26118  Cute funny Sex & Love Photography Watercooler....   \n",
       "13838  nobel peace prize Twitter U.S. World. Nobel Pe...   \n",
       "13708  adventure Video Videos Watercooler YouTube. 'H...   \n",
       "...                                                  ...   \n",
       "17678  Mobile Tech tweetdeck Twitter World. Teen Says...   \n",
       "1847   accessories Android The Future of Travel iPhon...   \n",
       "1846   Apps and Software art Dev & Design Fashion Gad...   \n",
       "17681  Advertising Business Evian Evian Babies Market...   \n",
       "4358   bluefin tuna Climate deepwater drilling deepwa...   \n",
       "\n",
       "                                             combi_text2 sentiment_pos  \\\n",
       "15271  happy holidays Social Media Vine Challenge. Yo...         0.737   \n",
       "26164  Gift guides holiday party Lifestyle Work & Pla...         0.701   \n",
       "26118  Cute funny Sex & Love Photography Watercooler....         0.692   \n",
       "13838  nobel peace prize Twitter U.S. World. Nobel Pe...         0.685   \n",
       "13708  adventure Video Videos Watercooler YouTube. 'H...         0.680   \n",
       "...                                                  ...           ...   \n",
       "17678  Mobile Tech tweetdeck Twitter World. Teen Says...         0.000   \n",
       "1847   accessories Android The Future of Travel iPhon...         0.000   \n",
       "1846   Apps and Software art Dev & Design Fashion Gad...         0.000   \n",
       "17681  Advertising Business Evian Evian Babies Market...         0.000   \n",
       "4358   bluefin tuna Climate deepwater drilling deepwa...         0.000   \n",
       "\n",
       "       sentiment_neu  sentiment_neg  sentiment_compound  \n",
       "15271          0.263          0.000              0.9413  \n",
       "26164          0.299          0.000              0.9371  \n",
       "26118          0.308          0.000              0.9584  \n",
       "13838          0.315          0.000              0.9545  \n",
       "13708          0.320          0.000              0.9313  \n",
       "...              ...            ...                 ...  \n",
       "17678          0.775          0.225             -0.2960  \n",
       "1847           1.000          0.000              0.0000  \n",
       "1846           1.000          0.000              0.0000  \n",
       "17681          1.000          0.000              0.0000  \n",
       "4358           0.893          0.107             -0.5719  \n",
       "\n",
       "[27643 rows x 28 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sort positive sentiment in ascending order\n",
    "df_train.loc[df_train['sentiment_pos'].sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f17bc08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf on 'combi_text2'('topic' + 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3729f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/chku/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## define preprocessor and tokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# define extra stopwords\n",
    "extra_stopwords = [\"ain't\", \"amn't\", \"aren't\", \"can't\", \"could've\", \"couldn't\",\n",
    "                    \"daresn't\", \"didn't\", \"doesn't\", \"don't\", \"gonna\", \"gotta\", \n",
    "                    \"hadn't\", \"hasn't\", \"haven't\", \"he'd\", \"he'll\", \"he's\", \"how'd\",\n",
    "                    \"how'll\", \"how's\", \"I'd\", \"I'll\", \"I'm\", \"I've\", \"isn't\", \"it'd\",\n",
    "                    \"it'll\", \"it's\", \"let's\", \"mayn't\", \"may've\", \"mightn't\", \n",
    "                    \"might've\", \"mustn't\", \"must've\", \"needn't\", \"o'clock\", \"ol'\",\n",
    "                    \"oughtn't\", \"shan't\", \"she'd\", \"she'll\", \"she's\", \"should've\",\n",
    "                    \"shouldn't\", \"somebody's\", \"someone's\", \"something's\", \"that'll\",\n",
    "                    \"that're\", \"that's\", \"that'd\", \"there'd\", \"there're\", \"there's\", \n",
    "                    \"these're\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this's\",\n",
    "                    \"those're\", \"tis\", \"twas\", \"twasn't\", \"wasn't\", \"we'd\", \"we'd've\",\n",
    "                    \"we'll\", \"we're\", \"we've\", \"weren't\", \"what'd\", \"what'll\", \n",
    "                    \"what're\", \"what's\", \"what've\", \"when's\", \"where'd\", \"where're\",\n",
    "                    \"where's\", \"where've\", \"which's\", \"who'd\", \"who'd've\", \"who'll\",\n",
    "                    \"who're\", \"who's\", \"who've\", \"why'd\", \"why're\", \"why's\", \"won't\",\n",
    "                    \"would've\", \"wouldn't\", \"y'all\", \"you'd\", \"you'll\", \"you're\", \n",
    "                    \"you've\", \"'s\", \"'d\", \"'m\", \"abov\", \"afterward\", \"ai\", \"alon\", \"alreadi\", \"alway\", \"ani\", \n",
    "                     \"anoth\", \"anyon\", \"anyth\", \"anywher\", \"becam\", \"becaus\", \"becom\", \"befor\", \n",
    "                     \"besid\", \"ca\", \"cri\", \"dare\", \"describ\", \"did\", \"doe\", \"dure\", \"els\", \n",
    "                     \"elsewher\", \"empti\", \"everi\", \"everyon\", \"everyth\", \"everywher\", \"fifti\", \n",
    "                     \"forti\", \"gon\", \"got\", \"henc\", \"hereaft\", \"herebi\", \"howev\", \"hundr\", \"inde\", \n",
    "                     \"let\", \"ll\", \"mani\", \"meanwhil\", \"moreov\", \"n't\", \"na\", \"need\", \"nobodi\", \"noon\", \n",
    "                     \"noth\", \"nowher\", \"ol\", \"onc\", \"onli\", \"otherwis\", \"ought\", \"ourselv\", \"perhap\", \n",
    "                     \"pleas\", \"sever\", \"sha\", \"sinc\", \"sincer\", \"sixti\", \"somebodi\", \"someon\", \"someth\", \n",
    "                     \"sometim\", \"somewher\", \"ta\", \"themselv\", \"thenc\", \"thereaft\", \"therebi\", \"therefor\", \n",
    "                     \"togeth\", \"twelv\", \"twenti\", \"ve\", \"veri\", \"whatev\", \"whenc\", \"whenev\", \n",
    "                    \"wherea\", \"whereaft\", \"wherebi\", \"wherev\", \"whi\", \"wo\", \"anywh\", \"el\", \"elsewh\", \"everywh\", \n",
    "                    \"ind\", \"otherwi\", \"plea\", \"somewh\", \"yourselv\"]\n",
    "\n",
    "stop = stop + extra_stopwords\n",
    "\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text\n",
    "\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48afa4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asteroid',\n",
       " 'asteroid',\n",
       " 'challeng',\n",
       " 'earth',\n",
       " 'space',\n",
       " 'u',\n",
       " 'world',\n",
       " 'nasa',\n",
       " 'grand',\n",
       " 'challeng',\n",
       " 'stop',\n",
       " 'asteroid',\n",
       " 'destroy',\n",
       " 'earth']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check tokenize, stemmization and non_stopwords\n",
    "tokenizer_stem_nostop(preprocessor(df_train['combi_text2'][0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc35eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define tf-idf vector\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocessor,  tokenizer=tokenizer_stem_nostop,\n",
    "                                  ngram_range=(1,1), \n",
    "                                  sublinear_tf = False,\n",
    "                                  dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f76f448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/2021_DL/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "world: 2.34\n",
      "tech: 2.57\n",
      "entertain: 2.68\n",
      "watercool: 2.70\n",
      "busi: 2.78\n",
      "u: 2.79\n",
      "video: 2.87\n",
      "app: 2.99\n",
      "softwar: 3.07\n",
      "mobil: 3.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/2021_DL/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "video: 1070.1407470703125\n",
      "world: 702.65478515625\n",
      "app: 695.5721435546875\n",
      "tech: 608.2487182617188\n",
      "busi: 598.9265747070312\n",
      "watercool: 545.3905639648438\n",
      "u: 519.1368408203125\n",
      "entertain: 511.7611389160156\n",
      "mobil: 472.13372802734375\n",
      "media: 462.6010437011719\n"
     ]
    }
   ],
   "source": [
    "## from lab code\n",
    "tfidf_vectorizer.fit(df_train['combi_text2'])\n",
    "\n",
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf_vectorizer.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' %(tfidf_vectorizer.get_feature_names()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "doc_tfidf = tfidf_vectorizer.transform(df_train['combi_text2']).toarray()\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf_vectorizer.inverse_transform(np.ones(tfidf_sum.shape[0]).reshape(1, -1))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "                        np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "511e0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply tf-idf on combi_text2 of training \n",
    "#all_topic = df_train['topic'].values.astype('U').tolist() + df_test['topic'].values.astype('U').tolist()\n",
    "train_tfidf_matrix = tfidf_vectorizer.fit_transform(df_train['combi_text2'].values.astype('U').tolist())\n",
    "\n",
    "#tfidf_matrix = tfidf_vectorizer.fit_transform(pd.concat(df_train['combi_text2'], df_test['combi_text2']), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "745f3f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 15724)\n"
     ]
    }
   ],
   "source": [
    "## check shape\n",
    "print(train_tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3cf21081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply hashing vector on combi_text2 of training and testing\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hashvec = HashingVectorizer(n_features=1024,\n",
    "                            preprocessor=preprocessor,\n",
    "                            tokenizer=tokenizer_stem_nostop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9d0b885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/2021_DL/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_hash_matrix = hashvec.fit_transform(df_train['combi_text2'].values.astype('U').tolist())\n",
    "test_hash_matrix = hashvec.fit_transform(df_test['combi_text2'].values.astype('U').tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a2f4e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27643, 1024)\n",
      "(11847, 1024)\n"
     ]
    }
   ],
   "source": [
    "## check shape\n",
    "print(train_hash_matrix.shape)\n",
    "print(test_hash_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1a87a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51934466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9244289",
   "metadata": {},
   "outputs": [],
   "source": [
    "##( or use another method ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96d6ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encode on ['media count'], ['img count']\n",
    "# 先不管這兩個features，可能要用EDA找特性\n",
    "##(ignore now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee2db1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\\n# one-hot-encode on 'weekday', 'author', 'hour', ...\\nOHE = OneHotEncoder(handle_unknown='ignore')\\ntrain_ohe_channel = OHE.fit_transform(df_train['weekday'].values.reshape(-1,1)).toarray()\\n\\n# OHE = OneHotEncoder(handle_unknown='ignore')\\ntest_ohe_channel = OHE.transform(df_test['channel'].values.reshape(-1,1)).toarray()\\nprint(train_ohe_channel.shape)\\nprint(test_ohe_channel.shape)\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先試試不要one-hot-encode已經處理過的資料，看效果如何\n",
    "##(ignore now)\n",
    "'''\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "# one-hot-encode on 'weekday', 'author', 'hour', ...\n",
    "OHE = OneHotEncoder(handle_unknown='ignore')\n",
    "train_ohe_channel = OHE.fit_transform(df_train['weekday'].values.reshape(-1,1)).toarray()\n",
    "\n",
    "# OHE = OneHotEncoder(handle_unknown='ignore')\n",
    "test_ohe_channel = OHE.transform(df_test['channel'].values.reshape(-1,1)).toarray()\n",
    "print(train_ohe_channel.shape)\n",
    "print(test_ohe_channel.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877d8fb",
   "metadata": {},
   "source": [
    "# so far, we process 'pub_date', 'channel' -> by observing EDA,  'author' -> by giving different weight by average popularity, ('topic', 'title') -> by sentimental detect and tf-idf\n",
    "# (Note: we haven't process 'Page Content' -> may turn into abstract, ('img count', 'media-count') -> may use EDA to exploring some properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3cf4d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'topic', 'channel', 'weekday', 'author', 'img count', 'title',\n",
       "       'media count', 'pub_date', 'num_words', 'Page Content', 'Popularity',\n",
       "       'day_of_month', 'month', 'day_of_week', 'hour', 'ymd', 'is_weekend',\n",
       "       'popular_month', 'popular_hour', 'popular_channel', 'popular_author',\n",
       "       'combi_text', 'combi_text2', 'sentiment_pos', 'sentiment_neu',\n",
       "       'sentiment_neg', 'sentiment_compound'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ec39e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop already processed \"numerical\" features + 一些還沒process的numerical features\n",
    "## (as ._get_numeric_data() will ignore categorical features, \\\n",
    "##  and all categorical features expect 'Page Content' are already processed)\n",
    "\n",
    "df_train_dropped = df_train.drop(['weekday',  'hour', 'month', 'day_of_week', 'day_of_month',\n",
    "                # not processed numerical features\n",
    "               'img count', 'media count', 'num_words', 'Id', ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c499bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_dropped = df_test.drop(['weekday',  'hour', 'month', 'day_of_week', 'day_of_month',\n",
    "                # not processed numerical features\n",
    "               'img count', 'media count', 'num_words', 'Id', ], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcca395d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['topic', 'channel', 'author', 'title', 'pub_date', 'Page Content',\n",
       "       'Popularity', 'ymd', 'is_weekend', 'popular_month', 'popular_hour',\n",
       "       'popular_channel', 'popular_author', 'combi_text', 'combi_text2',\n",
       "       'sentiment_pos', 'sentiment_neu', 'sentiment_neg',\n",
       "       'sentiment_compound'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_dropped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0fc8fbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['topic', 'channel', 'author', 'title', 'pub_date', 'Page Content',\n",
       "       'ymd', 'is_weekend', 'popular_month', 'popular_hour', 'popular_channel',\n",
       "       'popular_author', 'combi_text', 'combi_text2', 'sentiment_pos',\n",
       "       'sentiment_neu', 'sentiment_neg', 'sentiment_compound'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_dropped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aba2aefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>Page Content</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>ymd</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>popular_month</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>world</td>\n",
       "      <td>N/A</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>2013-06-19 15:04:30</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt; &lt;span c...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2013-06-19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>2013-03-28 17:40:55</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-03-28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>2014-05-07 19:15:20</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-05-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1043 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               topic        channel  \\\n",
       "0  Asteroid Asteroids challenge Earth Space U.S. ...          world   \n",
       "1  Apps and Software Google open source opn pledg...           tech   \n",
       "2      Entertainment NFL NFL Draft Sports Television  entertainment   \n",
       "\n",
       "             author                                              title  \\\n",
       "0               N/A  NASA's Grand Challenge: Stop Asteroids From De...   \n",
       "1  Christina Warren  Google's New Open Source Patent Pledge: We Won...   \n",
       "2         Sam Laird  Ballin': 2014 NFL Draft Picks Get to Choose Th...   \n",
       "\n",
       "              pub_date                                       Page Content  \\\n",
       "0  2013-06-19 15:04:30  <html><head><div class=\"article-info\"> <span c...   \n",
       "1  2013-03-28 17:40:55  <html><head><div class=\"article-info\"><span cl...   \n",
       "2  2014-05-07 19:15:20  <html><head><div class=\"article-info\"><span cl...   \n",
       "\n",
       "   Popularity         ymd  is_weekend  popular_month  ...  1014  1015  1016  \\\n",
       "0          -1  2013-06-19           0              0  ...   0.0   0.0   0.0   \n",
       "1           1  2013-03-28           0              1  ...   0.0   0.0   0.0   \n",
       "2           1  2014-05-07           0              0  ...   0.0   0.0   0.0   \n",
       "\n",
       "  1017 1018  1019  1020  1021  1022  1023  \n",
       "0  0.0  0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1  0.0  0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2  0.0  0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[3 rows x 1043 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>Page Content</th>\n",
       "      <th>ymd</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>popular_month</th>\n",
       "      <th>popular_hour</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Entertainment Music One Direction soccer Sports</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>Soccer Star Gets Twitter Death Threats After T...</td>\n",
       "      <td>2013-09-09 19:47:02</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gadgets glass Google Google Glass Google Glass...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Stan Schroeder</td>\n",
       "      <td>Google Glass Gets an Accessory Store</td>\n",
       "      <td>2013-10-31 09:25:02</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "      <td>2013-10-31</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazon amazon kindle Business Gaming</td>\n",
       "      <td>business</td>\n",
       "      <td>Todd Wasserman</td>\n",
       "      <td>OUYA Gaming Console Already Sold Out on Amazon</td>\n",
       "      <td>2013-06-25 12:54:54</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "      <td>2013-06-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1042 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               topic        channel  \\\n",
       "0    Entertainment Music One Direction soccer Sports  entertainment   \n",
       "1  Gadgets glass Google Google Glass Google Glass...           tech   \n",
       "2               amazon amazon kindle Business Gaming       business   \n",
       "\n",
       "           author                                              title  \\\n",
       "0       Sam Laird  Soccer Star Gets Twitter Death Threats After T...   \n",
       "1  Stan Schroeder               Google Glass Gets an Accessory Store   \n",
       "2  Todd Wasserman     OUYA Gaming Console Already Sold Out on Amazon   \n",
       "\n",
       "              pub_date                                       Page Content  \\\n",
       "0  2013-09-09 19:47:02  <html><head><div class=\"article-info\"><span cl...   \n",
       "1  2013-10-31 09:25:02  <html><head><div class=\"article-info\"><span cl...   \n",
       "2  2013-06-25 12:54:54  <html><head><div class=\"article-info\"><span cl...   \n",
       "\n",
       "          ymd  is_weekend  popular_month  popular_hour  ...  1014  1015 1016  \\\n",
       "0  2013-09-09           0              0             0  ...   0.0   0.0  0.0   \n",
       "1  2013-10-31           0             -1             0  ...   0.0   0.0  0.0   \n",
       "2  2013-06-25           0              0             0  ...   0.0   0.0  0.0   \n",
       "\n",
       "  1017      1018  1019  1020  1021  1022  1023  \n",
       "0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "1  0.0  0.164399   0.0   0.0   0.0   0.0   0.0  \n",
       "2  0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[3 rows x 1042 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# concat tf-idf matrix to df_train_dropped\n",
    "\n",
    "## error: cannot concat sparse matrix\n",
    "## solved: https://stackoverflow.com/questions/40570282/combine-sklearn-tfidf-with-additional-data\n",
    "\n",
    "## for tfidf 但太大了，在算corr()會卡很久\n",
    "#df_train_dropped_tfidf = pd.concat([df_train_dropped, pd.DataFrame(train_tfidf_matrix.toarray())], axis=1)\n",
    "\n",
    "## for hashvec\n",
    "df_train_dropped_hash = pd.concat([df_train_dropped, pd.DataFrame(train_hash_matrix.toarray())], axis=1)\n",
    "df_test_dropped_hash = pd.concat([df_test_dropped, pd.DataFrame(test_hash_matrix.toarray())], axis=1)\n",
    "display(df_train_dropped_hash.head(3))\n",
    "display(df_test_dropped_hash.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72a0a218",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Popularity</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>popular_month</th>\n",
       "      <th>popular_hour</th>\n",
       "      <th>popular_channel</th>\n",
       "      <th>popular_author</th>\n",
       "      <th>sentiment_pos</th>\n",
       "      <th>sentiment_neu</th>\n",
       "      <th>sentiment_neg</th>\n",
       "      <th>sentiment_compound</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.1481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.9153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1034 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Popularity  is_weekend  popular_month  popular_hour  popular_channel  \\\n",
       "0          -1           0              0             0               -2   \n",
       "1           1           0              1             0                2   \n",
       "2           1           0              0             0               -2   \n",
       "3          -1           0             -1             0                0   \n",
       "4          -1           0              0             0               -2   \n",
       "\n",
       "   popular_author  sentiment_pos  sentiment_neu  sentiment_neg  \\\n",
       "0               0          0.262          0.467          0.271   \n",
       "1               0          0.084          0.851          0.065   \n",
       "2               0          0.141          0.859          0.000   \n",
       "3               0          0.246          0.538          0.215   \n",
       "4               0          0.556          0.444          0.000   \n",
       "\n",
       "   sentiment_compound  ...  1014  1015  1016  1017  1018  1019  1020  1021  \\\n",
       "0             -0.2960  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "1              0.1481  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2              0.4215  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3              0.1027  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "4              0.9153  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   1022  1023  \n",
       "0   0.0   0.0  \n",
       "1   0.0   0.0  \n",
       "2   0.0   0.0  \n",
       "3   0.0   0.0  \n",
       "4   0.0   0.0  \n",
       "\n",
       "[5 rows x 1034 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>popular_month</th>\n",
       "      <th>popular_hour</th>\n",
       "      <th>popular_channel</th>\n",
       "      <th>popular_author</th>\n",
       "      <th>sentiment_pos</th>\n",
       "      <th>sentiment_neu</th>\n",
       "      <th>sentiment_neg</th>\n",
       "      <th>sentiment_compound</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.285</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.158</td>\n",
       "      <td>-0.4588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.8074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1033 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_weekend  popular_month  popular_hour  popular_channel  popular_author  \\\n",
       "0           0              0             0                0               0   \n",
       "1           0             -1             0                0               0   \n",
       "2           0              0             0                0               0   \n",
       "3           0              0             0                0               0   \n",
       "4           0             -1             0                0               0   \n",
       "\n",
       "   sentiment_pos  sentiment_neu  sentiment_neg  sentiment_compound    0  ...  \\\n",
       "0          0.119          0.596          0.285             -0.5994  0.0  ...   \n",
       "1          0.000          1.000          0.000              0.0000  0.0  ...   \n",
       "2          0.338          0.662          0.000              0.4767  0.0  ...   \n",
       "3          0.000          0.842          0.158             -0.4588  0.0  ...   \n",
       "4          0.304          0.696          0.000              0.8074  0.0  ...   \n",
       "\n",
       "   1014  1015  1016  1017      1018  1019  1020  1021  1022  1023  \n",
       "0   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0  0.164399   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0  0.000000   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 1033 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ignore categorical data and find top features\n",
    "\n",
    "## ._get_numeric_data() -> get numeric column\n",
    "\n",
    "#df_train_dropped_tfidf._get_numeric_data().head(5)\n",
    "display(df_train_dropped_hash._get_numeric_data().head(5))\n",
    "display(df_test_dropped_hash._get_numeric_data().head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8a73d89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popularity         1.000000\n",
      "popular_author     0.136841\n",
      "is_weekend         0.097561\n",
      "popular_month      0.064952\n",
      "popular_channel    0.044723\n",
      "                     ...   \n",
      "356                0.000048\n",
      "694                0.000044\n",
      "830                0.000041\n",
      "313                0.000037\n",
      "427                0.000012\n",
      "Name: Popularity, Length: 1034, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## find features according to corr() of each column to 'Popularity'\n",
    "\n",
    "## for tfidf\n",
    "\n",
    "#top_feats = abs(df_train_dropped_tfidf._get_numeric_data().corr()['Popularity']).sort_values(ascending=False).index[0:42]\n",
    "#print(abs(df_train_dropped_tfidf._get_numeric_data().corr()['Popularity']).sort_values(ascending=False))\n",
    "\n",
    "## for hashvec\n",
    "\n",
    "feats = abs(df_train_dropped_hash._get_numeric_data().corr()['Popularity']).sort_values(ascending=False)\n",
    "print(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "07f3fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose top correlated features\n",
    "num_top_feats = 42\n",
    "top_feats = feats.index[0:num_top_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8aa4aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some features has dropped \n",
    "#print([col for col in df_train_dropped_hash._get_numeric_data().columns if col not in top_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "379aadd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>topic</th>\n",
       "      <th>channel</th>\n",
       "      <th>weekday</th>\n",
       "      <th>author</th>\n",
       "      <th>img count</th>\n",
       "      <th>title</th>\n",
       "      <th>media count</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>num_words</th>\n",
       "      <th>...</th>\n",
       "      <th>popular_month</th>\n",
       "      <th>popular_hour</th>\n",
       "      <th>popular_channel</th>\n",
       "      <th>popular_author</th>\n",
       "      <th>combi_text</th>\n",
       "      <th>combi_text2</th>\n",
       "      <th>sentiment_pos</th>\n",
       "      <th>sentiment_neu</th>\n",
       "      <th>sentiment_neg</th>\n",
       "      <th>sentiment_compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>world</td>\n",
       "      <td>Wed</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1</td>\n",
       "      <td>NASA's Grand Challenge: Stop Asteroids From De...</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-19 15:04:30</td>\n",
       "      <td>536</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>Asteroid Asteroids challenge Earth Space U.S. ...</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.271</td>\n",
       "      <td>-0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>tech</td>\n",
       "      <td>Thu</td>\n",
       "      <td>Christina Warren</td>\n",
       "      <td>2</td>\n",
       "      <td>Google's New Open Source Patent Pledge: We Won...</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-03-28 17:40:55</td>\n",
       "      <td>305</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>Apps and Software Google open source opn pledg...</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.1481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Wed</td>\n",
       "      <td>Sam Laird</td>\n",
       "      <td>2</td>\n",
       "      <td>Ballin': 2014 NFL Draft Picks Get to Choose Th...</td>\n",
       "      <td>25</td>\n",
       "      <td>2014-05-07 19:15:20</td>\n",
       "      <td>1011</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television....</td>\n",
       "      <td>Entertainment NFL NFL Draft Sports Television....</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.4215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                              topic        channel  \\\n",
       "0   0  Asteroid Asteroids challenge Earth Space U.S. ...          world   \n",
       "1   1  Apps and Software Google open source opn pledg...           tech   \n",
       "2   2      Entertainment NFL NFL Draft Sports Television  entertainment   \n",
       "\n",
       "  weekday            author  img count  \\\n",
       "0     Wed               N/A          1   \n",
       "1     Thu  Christina Warren          2   \n",
       "2     Wed         Sam Laird          2   \n",
       "\n",
       "                                               title  media count  \\\n",
       "0  NASA's Grand Challenge: Stop Asteroids From De...            0   \n",
       "1  Google's New Open Source Patent Pledge: We Won...            0   \n",
       "2  Ballin': 2014 NFL Draft Picks Get to Choose Th...           25   \n",
       "\n",
       "              pub_date  num_words  ... popular_month  popular_hour  \\\n",
       "0  2013-06-19 15:04:30        536  ...             0             0   \n",
       "1  2013-03-28 17:40:55        305  ...             1             0   \n",
       "2  2014-05-07 19:15:20       1011  ...             0             0   \n",
       "\n",
       "   popular_channel  popular_author  \\\n",
       "0               -2               0   \n",
       "1                2               0   \n",
       "2               -2               0   \n",
       "\n",
       "                                          combi_text  \\\n",
       "0  Asteroid Asteroids challenge Earth Space U.S. ...   \n",
       "1  Apps and Software Google open source opn pledg...   \n",
       "2  Entertainment NFL NFL Draft Sports Television....   \n",
       "\n",
       "                                         combi_text2 sentiment_pos  \\\n",
       "0  Asteroid Asteroids challenge Earth Space U.S. ...         0.262   \n",
       "1  Apps and Software Google open source opn pledg...         0.084   \n",
       "2  Entertainment NFL NFL Draft Sports Television....         0.141   \n",
       "\n",
       "   sentiment_neu  sentiment_neg  sentiment_compound  \n",
       "0          0.467          0.271             -0.2960  \n",
       "1          0.851          0.065              0.1481  \n",
       "2          0.859          0.000              0.4215  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Popularity</th>\n",
       "      <th>popular_author</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>popular_month</th>\n",
       "      <th>popular_channel</th>\n",
       "      <th>310</th>\n",
       "      <th>500</th>\n",
       "      <th>820</th>\n",
       "      <th>736</th>\n",
       "      <th>279</th>\n",
       "      <th>...</th>\n",
       "      <th>765</th>\n",
       "      <th>259</th>\n",
       "      <th>52</th>\n",
       "      <th>354</th>\n",
       "      <th>239</th>\n",
       "      <th>482</th>\n",
       "      <th>459</th>\n",
       "      <th>816</th>\n",
       "      <th>sentiment_compound</th>\n",
       "      <th>719</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.158114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.316228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1481</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Popularity  popular_author  is_weekend  popular_month  popular_channel  \\\n",
       "0          -1               0           0              0               -2   \n",
       "1           1               0           0              1                2   \n",
       "2           1               0           0              0               -2   \n",
       "\n",
       "   310  500  820       736  279  ...  765  259   52       354  239  482  459  \\\n",
       "0  0.0  0.0  0.0  0.204124  0.0  ...  0.0  0.0  0.0  0.000000  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.158114  0.0  ...  0.0  0.0  0.0 -0.316228  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.000000  0.0  ...  0.0  0.0  0.0  0.000000  0.0  0.0  0.0   \n",
       "\n",
       "   816  sentiment_compound  719  \n",
       "0  0.0             -0.2960  0.0  \n",
       "1  0.0              0.1481  0.0  \n",
       "2  0.0              0.4215  0.0  \n",
       "\n",
       "[3 rows x 42 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popular_author</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>popular_month</th>\n",
       "      <th>popular_channel</th>\n",
       "      <th>310</th>\n",
       "      <th>500</th>\n",
       "      <th>820</th>\n",
       "      <th>736</th>\n",
       "      <th>279</th>\n",
       "      <th>507</th>\n",
       "      <th>...</th>\n",
       "      <th>765</th>\n",
       "      <th>259</th>\n",
       "      <th>52</th>\n",
       "      <th>354</th>\n",
       "      <th>239</th>\n",
       "      <th>482</th>\n",
       "      <th>459</th>\n",
       "      <th>816</th>\n",
       "      <th>sentiment_compound</th>\n",
       "      <th>719</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.5994</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.657596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   popular_author  is_weekend  popular_month  popular_channel  310  500  820  \\\n",
       "0               0           0              0                0  0.0  0.0  0.0   \n",
       "1               0           0             -1                0  0.0  0.0  0.0   \n",
       "2               0           0              0                0  0.0  0.0  0.0   \n",
       "\n",
       "   736  279  507  ...  765  259   52       354  239  482  459  816  \\\n",
       "0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  ...  0.0  0.0  0.0 -0.657596  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   sentiment_compound  719  \n",
       "0             -0.5994  0.0  \n",
       "1              0.0000  0.0  \n",
       "2              0.4767  0.0  \n",
       "\n",
       "[3 rows x 41 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# drop processed features and concat tf-idf\n",
    "df_train_processed = df_train_dropped_hash[top_feats]\n",
    "df_test_processed = df_test_dropped_hash[top_feats[1:]] # should not contain 'Popularity'\n",
    "display(df_train.head(3))\n",
    "display(df_train_processed.head(3))\n",
    "display(df_test_processed.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5b54eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output\n",
    "df_train_processed.to_csv('../chk_output/train_processed.csv', index=False)\n",
    "df_test_processed.to_csv('../chk_output/test_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b669a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644ea3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tf_2021DL)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
